<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mysql on joshuaprunier.com</title>
    <link>http://joshuaprunier.com/tags/mysql/</link>
    <description>Recent content in Mysql on joshuaprunier.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <copyright>Copyright (c) 2015</copyright>
    <lastBuildDate>Fri, 14 Feb 2014 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://joshuaprunier.com/tags/mysql/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Introducing Trite: A tool for automating import of InnoDB tablespaces</title>
      <link>http://joshuaprunier.com/blog/2014/trite_intro/post/</link>
      <pubDate>Fri, 14 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://joshuaprunier.com/blog/2014/trite_intro/post/</guid>
      <description>&lt;p&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;
  &lt;a href=&#34;#&#34; imageanchor=&#34;1&#34; style=&#34;clear: left; float: left; margin-bottom: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://joshuaprunier.com/static/img/gopher_dolphin.jpg&#34;   /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;
Mysqldump is a fantastic tool for backing up and restoring small and medium sized MySQL tables and databases quickly. However, when databases surge into the multi-terabyte range restoring from logical backups is inefficient. It can take a significant amount of time to insert a hundred million plus rows to a single table, even with very fast I/O. Programs like MySQL Enterprise Backup and Percona XtraBackup allow non-blocking binary copies of your InnoDB tables to be taken while it is online and processing requests. XtraBackup also has an export feature that allows InnoDB file per tablespaces to be detached from the shared table space and imported to a completely different MySQL instance.
&lt;/p&gt;

&lt;p&gt;
The necessary steps to export and import InnoDB tables are in the XtraBackup documentation &lt;a href=&#34;http://www.percona.com/doc/percona-xtrabackup/2.1/innobackupex/importing_exporting_tables_ibk.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Although fairly simple and straight forward it requires a bit too much typing if you want to move more than a couple of tables. To automate the process I created a new open source project named &lt;a href=&#34;https://github.com/joshuaprunier/trite&#34; target=&#34;_blank&#34;&gt;Trite&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Trite stands for &lt;b&gt;TR&lt;/b&gt;ansport &lt;b&gt;I&lt;/b&gt;nnodb &lt;b&gt;T&lt;/b&gt;ables &lt;b&gt;E&lt;/b&gt;fficiently and is also a nod to the repetitive manual steps required to move tablespaces. Trite is a client/server written in 100% &lt;a href=&#34;http://golang.org/&#34; target=&#34;_blank&#34;&gt;Go&lt;/a&gt; that leverages XtraBackups&#39;s import/export feature and aids a DBA in creating, recovering or refreshing databases. The initial problem that led me to create Trite is a common development database environment where I work. Multiple applications share the same database servers which requires each applications data be refreshed at different intervals. Before Trite we used mysqldump and it took the majority of a weekend for the data refresh to complete, even with good hardware and my.cnf&#39;s tuned for performance. With Trite we get the same results in a fraction of the time!
&lt;/p&gt;

&lt;p&gt;
Trite has three modes of operation: dump, server and client. Dump mode writes out create statements for schemas, tables, stored procedures, functions, triggers and views. Server mode makes the XtraBackup and dump create files accessible via a Go powered HTTP server. Client mode downloads the create statements and XtraBackup files from a Trite server to recreate database objects.
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;p&gt;
Twelve gigabytes of &lt;a href=&#34;http://imdbpy.sourceforge.net/&#34; target=&#34;_blank&#34;&gt;IMDB&lt;/a&gt; data will make a suitably sized database for an example. The hardware used is a woefully under powered laptop; 4 gigs of RAM, 32bit CPU, slow SATA hard drive. Before each data load the imdb schema was removed and MySQL restarted to wipe caches. It took just over two hours to restore from the dump file. This is with innodb_doublewrite turned off to try and speed up all the inserts.
&lt;/p&gt;&lt;/p&gt;

&lt;pre&gt;
jprunier@beefeater:~/Downloads$ ls -alh imdb.sql
-rw-rw-r-- 1 jprunier jprunier 4.8G Jan 24 18:41 imdb.sql

jprunier@beefeater:~/Downloads$ time mysql -uroot -p &lt; imdb.sql
Enter password: 

real    130m55.167s
user    2m16.052s
sys     0m9.020s

jprunier@beefeater:~/Downloads$ sudo du -sh /var/lib/mysql/imdb
[sudo] password for jprunier: 
12G /var/lib/mysql/imdb
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;p&gt;
A binary backup of the database took 8 minutes with XtraBackup.
&lt;/p&gt;&lt;/p&gt;

&lt;pre&gt;
root@beefeater:~# time innobackupex --user=root --password= /root

... output omitted ...

innobackupex: Backup created in directory &#39;/root/2014-01-25_14-22-06&#39;
140125 14:30:29  innobackupex: Connection to database server closed
140125 14:30:29  innobackupex: completed OK!

real    8m23.118s
user    0m42.076s
sys     0m31.524s
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;p&gt;
And 16 seconds to prepare the tables for export. This will take longer depending on the amount of changes the database receives during the backup. The apply can be sped up by giving the innobackupex process more memory.
&lt;/p&gt;&lt;/p&gt;

&lt;pre&gt;
root@beefeater:~# time innobackupex --apply-log --use-memory=1G --export /root/2014-01-25_14-22-06

... output omitted ...

140125 14:33:03  InnoDB: Starting shutdown...
140125 14:33:07  InnoDB: Shutdown completed; log sequence number 70808870412
140125 14:33:07  innobackupex: completed OK!

real    0m16.310s
user    0m0.900s
sys     0m0.620s
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;p&gt;
A dump of the database objects should be done as close to the backup as possible. If the Trite create statement from a dump differs significantly from the copy of table taken with XtraBackup the table may fail to import.
&lt;/p&gt;&lt;/p&gt;

&lt;pre&gt;
root@beefeater:~# ./trite -dump -user=root -password= -host=localhost
Dumping to: /root/localhost_dump20140125173337

Enter password: 

imdb: 21 tables, 0 procedures, 0 functions, 0 triggers, 0 views

22 total objects dumped

Total runtime = 2.19331264s
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;p&gt;
In this example the Trite server is being run on the same machine as the database being restored. If your database is very large the backup, create dumps and Trite server will probably be on a different host.
&lt;/p&gt;&lt;/p&gt;

&lt;pre&gt;
root@beefeater:~# screen -S trite_server
root@beefeater:~# ./trite -server -dump_path=/root/localhost_dump20140125173337 -backup_path=/root/2014-01-25_14-22-06

Starting server listening on port 12000
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;p&gt;
The Trite client was able to restore the imdb database in 13 minutes. Much faster than doing all those inserts over with mysqldump. You can run the client with multiple worker threads, the default is single threaded. The optimal number of workers depends on the size of your tables, how fast your disks are and how fast your network is if the server is remote. In the case of the laptop, multiple workers actually makes the restore time slower.
&lt;/p&gt;&lt;/p&gt;

&lt;pre&gt;
root@beefeater:~# ./trite -client -user=root -password= -socket=/var/lib/mysql/mysql.sock -server_host=localhost
Enter password: 
imdb.link_type has been restored
imdb.comp_cast_type has been restored
imdb.movie_info has been restored
imdb.aka_name has been restored
imdb.complete_cast has been restored
imdb.movie_link has been restored
imdb.company_name has been restored
imdb.char_name has been restored
imdb.name has been restored
imdb.person_info has been restored
imdb.keyword has been restored
imdb.cast_info has been restored
imdb.role_type has been restored
imdb.aka_title has been restored
imdb.kind_type has been restored
imdb.movie_info_idx has been restored
imdb.movie_keyword has been restored
imdb.company_type has been restored
imdb.info_type has been restored
imdb.title has been restored
imdb.movie_companies has been restored
Applying triggers for imdb
Applying views for imdb
Applying procedures for imdb
Applying functions for imdb

Total runtime = 13m3.881954844s
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;p&gt;
Xtrabackup can restore the entire database just as quick but you need to restore the entire backup. The power of Trite is being able to customize a restore. By removing files from the dump directory you can restore just spcific schemas or tables. You can also restore to a database that contains tables while it is online and responding to queries. Some of the features I am planning on adding next are compression for transfer across slow networks and replication support so only objects conforming to a slaves replication filters will be applied.
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;p&gt;
Trite does have some limitations. The full list can be found on the &lt;a href=&#34;http://github.com/joshuaprunier/trite&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; page but the top ones are:
&lt;/p&gt;&lt;/p&gt;

&lt;div&gt;
  &lt;ul&gt;
    &lt;li&gt;InnoDB file per table is required.&lt;/li&gt;
    &lt;li&gt;The database you are restoring to must be running Percona server 5.1, 5.5, 5.6 or Oracle MySQL 5.6 or MariaDB 5.5, 10.&lt;/li&gt;
    &lt;li&gt;Windows is not currently supported due to a few Linux specific lines of code.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;
If you think Trite might benefit you and your organization give it a try and send me any feedback you might have. I have tried hard to make Trite efficient and bug free but there might be issues when used with certain configurations. Be sure to test Trite on non-critical databases before use in production. If you encounter any bugs please open an issue via GitHub.
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DDL statements in MySQL 5.x with row-based replication</title>
      <link>http://joshuaprunier.com/blog/2013/ddl_statements/post/</link>
      <pubDate>Sun, 02 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>http://joshuaprunier.com/blog/2013/ddl_statements/post/</guid>
      <description>&lt;p&gt;In the replication topology I manage there are many layers of replication filters that prune data at the database and&amp;nbsp;in a few places table level. The way MySQL replicates &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.5/en/sql-syntax-data-definition.html&#34;&gt;Data Definition Language&lt;/a&gt; (create, alter, drop) statements differs from how &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.5/en/sql-syntax-data-manipulation.html&#34;&gt;Data Manipulation Language&lt;/a&gt; (insert, update, delete) statements are handled with row-based replication. I often need to fix broken replication due to a lack of understanding of these subtle differences.&lt;/p&gt;

&lt;p&gt;With row-based replication DML statements focus directly on the table being modified. DDL on the other hand always uses statement-based replication and is tied to what is known in MySQL as the &amp;ldquo;default database&amp;rdquo;. The default database is the schema/database currently in use when a DDL statement is executed. The only information I was able to find regarding this in the MySQL documentation is the second to last paragraph &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.5/en/binary-log-setting.html&#34;&gt;here&lt;/a&gt;. If do-db or ignore-db filters are configured on slaves, replication can break when DDL statements are used in certain scenarios. Table level or wildcard replication filters can also add to the potential breakage but I will save that for a separate blog post.&lt;/p&gt;

&lt;p&gt;A few examples should make things clear. The slave in the following examples is configured with &amp;ldquo;replicate-ignore-db = test&amp;rdquo;. That means it will replicate all objects from its master except for anything in the &amp;ldquo;test&amp;rdquo; schema/database. The test environment is reset between examples. I have changed the prompt via the my.cnf to show the default database and statement counter: prompt = &amp;lsquo;\h(\d):\c&amp;gt; &amp;lsquo;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;h4&gt;Example #1&lt;/h4&gt;&lt;/p&gt;

&lt;p&gt;On the master we use the test schema and create a table named ddl_test.&lt;/p&gt;

&lt;pre&gt;
master((none)):1&gt; use test;
Database changed
master(test):2&gt; show tables;
Empty set (0.00 sec)

master(test):3&gt; create table ddl_test(id tinyint unsigned auto_increment primary key);
Query OK, 0 rows affected (0.27 sec)

master(test):4&gt; show tables;
+----------------+
| Tables_in_test |
+----------------+
| ddl_test       |
+----------------+
1 row in set (0.00 sec)

master(test):5&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
The ddl_test table does not replicate to the test schema which is the expected behavior.&lt;/p&gt;

&lt;pre&gt;
slave((none)):1&gt; use test;
Database changed
slave(test):2&gt; show tables;
Empty set (0.00 sec)

slave(test):3&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;h4&gt;
Example #2&lt;/h4&gt;&lt;/p&gt;

&lt;p&gt;This time on the master we use the world schema, create the ddl_test table in the test schema by way of a fully qualified table name and insert a row to it.&lt;/p&gt;

&lt;pre&gt;
master((none)):1&gt; use world;
Database changed
master(world):2&gt; create table test.ddl_test(id tinyint unsigned auto_increment primary key);
Query OK, 0 rows affected (0.28 sec)

master(world):3&gt; insert into test.ddl_test(id) values(1);
Query OK, 1 row affected (0.00 sec)

master(world):4&gt; use test;
Database changed
master(test):5&gt; show tables;
+----------------+
| Tables_in_test |
+----------------+
| ddl_test       |
+----------------+
1 row in set (0.00 sec)

master(test):6&gt; select * from ddl_test;
+----+
| id |
+----+
|  1 |
+----+
1 row in set (0.00 sec)

master(test):7&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
In this case the table creation is replicated because create is a DDL statement and the world schema was the default database when the statement was issued. Even though the default database was world, an insert does not replicate to the slave because it is a DML statement and the slave is configured to ignore events on objects in the test schema.&lt;/p&gt;

&lt;pre&gt;
slave((none)):1&gt; use test;
Database changed
slave(test):2&gt; show tables;
+----------------+
| Tables_in_test |
+----------------+
| ddl_test       |
+----------------+
1 row in set (0.00 sec)

slave(test):3&gt; select * from ddl_test;
Empty set (0.00 sec)

slave(test):4&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;h4&gt;
Example #3&lt;/h4&gt;&lt;/p&gt;

&lt;p&gt;Now we create the ddl_test table while test is the default database, then alter the table to add a column while world is the default database.&lt;/p&gt;

&lt;pre&gt;
master((none)):1&gt; use test;
Database changed
master(test):2&gt; create table test.ddl_test(id tinyint unsigned auto_increment primary key);
Query OK, 0 rows affected (0.30 sec)

master(test):3&gt; use world;
Database changed
master(world):4&gt; alter table test.ddl_test add column newcol varchar(1);
Query OK, 0 rows affected (0.48 sec)
Records: 0  Duplicates: 0  Warnings: 0

master(world):5&gt; show create table test.ddl_test\G
*************************** 1. row ***************************
       Table: ddl_test
Create Table: CREATE TABLE `ddl_test` (
  `id` tinyint(3) unsigned NOT NULL AUTO_INCREMENT,
  `newcol` varchar(1) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1
1 row in set (0.06 sec)

master(world):6&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
Ouch! The result is broken replication on the slave. The reason is the original table create did not replicate but the additional alter statement does.&lt;/p&gt;

&lt;pre&gt;
slave((none)):1&gt; use test;
Database changed
slave(test):2&gt; show tables;
Empty set (0.00 sec)

slave(test):3&gt; show slave status\G
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 192.168.1.101
                  Master_User: root
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: master-bin.000002
          Read_Master_Log_Pos: 1132
               Relay_Log_File: slave-relay-bin.000010
                Relay_Log_Pos: 1164
        Relay_Master_Log_File: master-bin.000002
             Slave_IO_Running: Yes
            Slave_SQL_Running: No
              Replicate_Do_DB: 
          Replicate_Ignore_DB: test
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 1146
                   Last_Error: Error &#39;Table &#39;test.ddl_test&#39; doesn&#39;t exist&#39; on query. Default database: &#39;world&#39;. Query: &#39;alter table test.ddl_test add column newcol varchar(1)&#39;
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 1014
              Relay_Log_Space: 1589
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: NULL
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 1146
               Last_SQL_Error: Error &#39;Table &#39;test.ddl_test&#39; doesn&#39;t exist&#39; on query. Default database: &#39;world&#39;. Query: &#39;alter table test.ddl_test add column newcol varchar(1)&#39;
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 1
1 row in set (0.00 sec)

slave(test):4&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
The error message shows exactly what went wrong. Table does not exist is pretty clear. The default database is world, the query references the test.ddl_test table and shows the alter table command that was attempted. Show slave status output also shows us the Replicate_Ignore_DB configuration has been set on the slave to ignore all DDL and DML statements against the test schema.&lt;/p&gt;

&lt;p&gt;The next course of action is to get replication going again. We have deduced the alter table statement should not have replicated so we simply skip the event using the sql_slave_skip_counter variable.&lt;/p&gt;

&lt;pre&gt;
slave(test):4&gt; set global sql_slave_skip_counter=1;
Query OK, 0 rows affected (0.00 sec)

slave(test):5&gt; start slave;
Query OK, 0 rows affected (0.00 sec)

slave(test):6&gt; show slave status\G
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 192.168.1.101
                  Master_User: root
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: master-bin.000002
          Read_Master_Log_Pos: 1132
               Relay_Log_File: slave-relay-bin.000010
                Relay_Log_Pos: 1282
        Relay_Master_Log_File: master-bin.000002
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: test
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 1132
              Relay_Log_Space: 1589
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 0
               Last_SQL_Error: 
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 1
1 row in set (0.00 sec)

slave(test):7&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
Not directly related to the theme of this post but worth mentioning is the unique schema in MySQL known as none or null. This schema can not be used directly since it is the absence of having a default database. The only way I know of acquiring null schema is to connect to mysql without the -D option or by dropping the schema that is currently in use. How DDL and DML statements are handled remains the same with or without a default database.&lt;/p&gt;

&lt;p&gt;Hopefully the examples above make the rules that govern replication of DDL and DML statements clearer. The potential impact on a dba really depends on the size and configuration of their replication topology. The more databases and filters in place to limit and/or direct what is replicated, the greater the chance replication can break. Number of users manipulating database objects, their knowledge of MySQL and the replication topology they are working within is also a big factor.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Delayed row-based replication with large tables lacking a primary key</title>
      <link>http://joshuaprunier.com/blog/2013/delayed_replication/post/</link>
      <pubDate>Mon, 13 May 2013 00:00:00 +0000</pubDate>
      
      <guid>http://joshuaprunier.com/blog/2013/delayed_replication/post/</guid>
      <description>&lt;p&gt;I configure all our master databases to use row-based binary logging where I work. In my opinion it is a much safer option than statement-based replication. The advantages and disadvantages of both types of MySQL replication are detailed in the online documentation&amp;nbsp;&lt;a href=&#34;https://dev.mysql.com/doc/refman/5.5/en/replication-sbr-rbr.html&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. You can&amp;rsquo;t view the events a slave is applying directly with &lt;i&gt;&lt;b&gt;&amp;lsquo;show processlist&amp;rsquo;&lt;/b&gt;&lt;/i&gt; but by issuing &lt;i&gt;&lt;b&gt;&amp;lsquo;show open tables where in use&amp;rsquo;&lt;/b&gt;&lt;/i&gt; you can detect what table is receiving the attention of the SQL thread. If you need more information the mysqlbinlog command must be used to decode the slaves relay logs or masters binary logs.&lt;/p&gt;

&lt;p&gt;Our developers often change a lot of rows with a single update statement. This usually results in some reasonable replication lag on downstream slaves.&amp;nbsp;Occasionally&amp;nbsp;the lag continues to grow and eventually nagios complains. Investigating the lag I sometimes discover the root of the problem is due to millions of rows updated on a table with no primary key. Putting a primary key constraint on a table is just good practice, especially on InnoDB tables. This is really necessary for any large table that will be replicated.&lt;/p&gt;

&lt;p&gt;To show what replication is actually doing I have cloned the sakila.film table, omitting the indexes, inserted all its rows and finally updated the description column for all 1,000 rows in the new table.&lt;/p&gt;

&lt;pre&gt;
beefeater(test)&gt; show master status;
+----------------------+----------+--------------+------------------+
| File                 | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+----------------------+----------+--------------+------------------+
| beefeater-bin.000001 |      107 |              |                  |
+----------------------+----------+--------------+------------------+
1 row in set (0.00 sec)

beefeater(test)&gt; use test;
Database changed
beefeater(test)&gt; CREATE TABLE test.film (
    -&gt;   film_id smallint(5) unsigned NOT NULL,
    -&gt;   title varchar(255) NOT NULL,
    -&gt;   description text,
    -&gt;   release_year year(4) DEFAULT NULL,
    -&gt;   language_id tinyint(3) unsigned NOT NULL,
    -&gt;   original_language_id tinyint(3) unsigned DEFAULT NULL,
    -&gt;   rental_duration tinyint(3) unsigned NOT NULL DEFAULT &#39;3&#39;,
    -&gt;   rental_rate decimal(4,2) NOT NULL DEFAULT &#39;4.99&#39;,
    -&gt;   length smallint(5) unsigned DEFAULT NULL,
    -&gt;   replacement_cost decimal(5,2) NOT NULL DEFAULT &#39;19.99&#39;,
    -&gt;   rating enum(&#39;G&#39;,&#39;PG&#39;,&#39;PG-13&#39;,&#39;R&#39;,&#39;NC-17&#39;) DEFAULT &#39;G&#39;,
    -&gt;   special_features set(&#39;Trailers&#39;,&#39;Commentaries&#39;,&#39;Deleted Scenes&#39;,&#39;Behind the Scenes&#39;) DEFAULT NULL,
    -&gt;   last_update timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
    -&gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8;
Query OK, 0 rows affected (0.23 sec)

beefeater(test)&gt; insert into test.film select * from sakila.film;
Query OK, 1000 rows affected (0.37 sec)
Records: 1000  Duplicates: 0  Warnings: 0

beefeater(test)&gt; show master status;
+----------------------+----------+--------------+------------------+
| File                 | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+----------------------+----------+--------------+------------------+
| beefeater-bin.000001 |   137389 |              |                  |
+----------------------+----------+--------------+------------------+
1 row in set (0.00 sec)

beefeater(test)&gt; update test.film set description = &#39;&#39;;
Query OK, 1000 rows affected (0.05 sec)
Rows matched: 1000  Changed: 1000  Warnings: 0

beefeater(test)&gt; show master status;
+----------------------+----------+--------------+------------------+
| File                 | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+----------------------+----------+--------------+------------------+
| beefeater-bin.000001 |   313847 |              |                  |
+----------------------+----------+--------------+------------------+
1 row in set (0.00 sec)
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
I have edited the output of mysqlbinlog to only show the entries related to the first row created by the insert and update&amp;nbsp;statements&amp;nbsp;above. The&amp;nbsp;@ symbol followed by a number is a mapping to the columns in the table. So&amp;nbsp;@1=film_id,&amp;nbsp;@2=title, @3=description and so on. Note that the update statement records a before and after picture of the row. This is can be used in a pinch to fix mistaken updates if the damage is small instead of having to restore from backups.&lt;/p&gt;

&lt;pre&gt;
mysqlbinlog --base64-output=auto beefeater-bin.000001 | more
...
### INSERT INTO test.film
### SET
###   @1=1
###   @2=&#39;ACADEMY DINOSAUR&#39;
###   @3=&#39;A Epic Drama of a Feminist And a Mad Scientist who must Battle a Teacher in The Canadian R
ockies&#39;
###   @4=2006
###   @5=1
###   @6=NULL
###   @7=6
###   @8=990000000
###   @9=86
###   @10=000000020.990000000
###   @11=2
###   @12=b&#39;00001100&#39;
###   @13=1139997822
...

root@beefeater:/var/lib/mysql# mysqlbinlog --base64-output=auto --start-position=137389 beefeater-bin.000001 | more
...
### UPDATE test.film
### WHERE
###   @1=1
###   @2=&#39;ACADEMY DINOSAUR&#39;
###   @3=&#39;A Epic Drama of a Feminist And a Mad Scientist who must Battle a Teacher in The Canadian R
ockies&#39;
###   @4=2006
###   @5=1
###   @6=NULL
###   @7=6
###   @8=990000000
###   @9=86
###   @10=000000020.990000000
###   @11=2
###   @12=b&#39;00001100&#39;
###   @13=1139997822
### SET
###   @1=1
###   @2=&#39;ACADEMY DINOSAUR&#39;
###   @3=&#39;&#39;
###   @4=2006
###   @5=1
###   @6=NULL
###   @7=6
###   @8=990000000
###   @9=86
###   @10=000000020.990000000
###   @11=2
###   @12=b&#39;00001100&#39;
###   @13=1368396106
...
&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
So row-based replication is performing as named and creating a binary log event for each row affected. My single insert and update statement on the master then became 1,000 separate events on the slave.&lt;/p&gt;

&lt;p&gt;Digging in to the MySQL source code I was unable to confirm exactly how the SQL thread applies relay log events on a slave. I assume it is similar to what happens when a normal update statement is executed on a table with no indexes. The server must perform a full table scan to locate a single row. For a table with a million plus rows a million full table scans is expensive! A primary key or suitable unique index will prevent this type of problem.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>